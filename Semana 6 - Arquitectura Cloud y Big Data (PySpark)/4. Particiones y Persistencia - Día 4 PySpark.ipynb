{"cells":[{"cell_type":"markdown","source":["### PARTICIONES"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d60ce9c7-7c05-450e-acfa-f04b4dbb47a0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Los datos se dividen en particiones que se distribuyen en la memoria de distintos nodos (nodos workers)\n\n# Se intenta que dicho reparto se efectúe de la forma más óptima posible\n\n# El Shuffle (movimiento de datos en el clúster) puede llegar a ser un problema en cuanto a optimización\n\n# En PySpark, 'coalesce' y 'repartition' son operaciones que permiten reorganizar la distribución de los datos en los nodos del clúster."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"503372fc-dc34-4bf7-b74d-a8246e14c846","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Se utiliza para reducir el número de particiones de un RDD (a las que indiques) y crea un RDD nuevo (suele ser más eficiente).\n# Minimiza el movimiento de datos; pero no obtiene particiones homogéneas.\n\nrdd = sc.parallelize([3, 2, 1, 4, 5, 7, 9, 6, 2, 8], 4)\nrdd2 = rdd.coalesce(3)\n\n    # Informativo del resultado\nprint(rdd2.getNumPartitions())  # Devuelve número de particiones\nprint(rdd2.glom().collect())    # Crea RDD cuyos elementos son las particiones"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"9cc14091-49d7-44f2-959a-fc9f92fa187f","inputWidgets":{},"title":"Coalesce()"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["3\n[[3, 2], [1, 4], [5, 7, 9, 6, 2, 8]]\n"]}],"execution_count":0},{"cell_type":"code","source":["# Puede aumentar o reducir el número de  particiones del RDD (crea uno nuevo).\n# Particiones resultantes son de igual tamaño (aprox.), pero produce shuffle\n\nrdd = sc.parallelize([4, 5, 7, 9, 6, 2, 8, 3], 3)\nrdd2 = rdd.repartition(4)\n\n    # Informativo del resultado\nprint(rdd2.getNumPartitions())  # Devuelve número de particiones\nprint(rdd2.glom().collect())    # Crea RDD cuyos elementos son las particiones"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"2e15464b-8c54-4573-9604-1ad9c11c2a96","inputWidgets":{},"title":"Repartition()"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["4\n[[6, 2, 8, 3], [7, 9], [], [4, 5]]\n"]}],"execution_count":0},{"cell_type":"code","source":["# Al final la diferencia radica en que hay dos variables a optimizar:\n    # Homogeneidad de las particiones (más homogéneo, mejor)\n    # Shuffle: movimiento de datos en el clúster (menos movimiento, mejor)\n    \n# Coalesce: Menos homogéneo (malo), menos shuffle (bueno)\n\n# Repartition: Más homogéneo (bueno), más shuffle (malo)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f18c192c-4982-425f-b3a8-782cbc7456bf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Crear un RDD a partir de un archivo\nrdd = sc.parallelize([3, 2, 1, 4, 5, 7, 9, 6, 2, 8])\n\n# Contar el número de líneas antes de la reducción de particiones\nprint(\"Antes de reducir particiones:\", rdd.getNumPartitions())\n\n# Reducir el número de particiones utilizando coalesce\nrdd_coalesced = rdd.coalesce(2)\n\n# Contar el número de líneas después de la reducción de particiones\nprint(\"Después de reducir particiones con coalesce:\", rdd_coalesced.getNumPartitions())\n\n# Redistribuir los datos utilizando repartition\nrdd_repartitioned = rdd.repartition(4)\n\n# Contar el número de líneas después de la redistribución de particiones\nprint(\"Después de redistribuir particiones con repartition:\", rdd_repartitioned.getNumPartitions())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"0544f48c-5cae-4d76-ae01-2be85fba4990","inputWidgets":{},"title":"Ejemplo diferencia"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Antes de reducir particiones: 8\nDespués de reducir particiones con coalesce: 2\nDespués de redistribuir particiones con repartition: 4\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### PERSISTENCIA"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23e22dfe-5b55-483f-8a00-bdd3876981b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# La persistencia en PySpark se refiere a la capacidad de almacenar en caché los resultados de una operación en memoria o disco para su reutilización posterior. Mejora el rendimiento. Es decir, en lugar de calcularlo para cada vez que se pide, guarda el resultado y lo da tal cual cuando se pide.\n\n# Se puede utilizar persist() o cache().\n\n# Liberar la memoria caché: .unpersist() cuando ya no sea necesario."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b087b518-bf50-4b63-8db0-40ab7c9dc3aa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# '<>' puede ser:\n    # MEMORY_ONLY (por defecto)\n    # DISK_ONLY (solo disco)\n    # MEMORY_AND_DISK (1º memoria, y si no cabe, disco)\n\nfrom pyspark import StorageLevel\n\nfile = 'dbfs:/FileStore/shared_uploads/danielmm97@gmail.com/quijote.txt'\nlineas = sc.textFile(file)\nlong_lineas = lineas.map(lambda elemento: len(elemento))\nlong_lineas.persist(StorageLevel.MEMORY_ONLY) \t\t\t\t\n\nprint(long_lineas.reduce(lambda elem1, elem2: elem1 + elem2))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"e1e205f0-2f22-4638-ada2-adcfccaf8fe6","inputWidgets":{},"title":"Persist(StorageLevel.<>)"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["2034318\n"]}],"execution_count":0},{"cell_type":"code","source":["# cache() es una versión abreviada de persist() porque tiene el parámetro predeterminado seteado en MEMORY_ONLY.\n    # rdd.persist(StorageLevel.MEMORY_ONLY) == rdd.cache()\n\n# Si se desea especificar un nivel de almacenamiento diferente a MEMORY_ONLY, no puede usarse cache() es necesario utilizar persist(...) con el argumento correspondiente.\n\nfile = 'dbfs:/FileStore/shared_uploads/danielmm97@gmail.com/quijote.txt'\nlineas = sc.textFile(file)\nlong_lineas = lineas.flatMap(lambda elemento: elemento.split())\nlong_lineas.cache() \n \nprint(long_lineas.count())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"664ba4c3-fb52-4dfe-9f4a-3257537ca35e","inputWidgets":{},"title":"Cache()"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["376500\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"4. Particiones y Persistencia - Día 4 PySpark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":190124382011162}},"nbformat":4,"nbformat_minor":0}
